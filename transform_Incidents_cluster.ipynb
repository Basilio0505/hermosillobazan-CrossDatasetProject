{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoFn to perform on each element in the input PCollection.\n",
    "class CreatePrimaryKey(beam.DoFn):\n",
    "\n",
    "    def process(self, element):\n",
    "        record = element\n",
    "        incident_id = record.get('serialid')   \n",
    "        state = record.get('state')\n",
    "        fdid = record.get('fdid')\n",
    "        date = record.get('date')\n",
    "        inc_no = record.get('inc_no')\n",
    "        exposure_no = record.get('exposure_no')\n",
    "        weath_type = record.get('weath_type')\n",
    "        wind_dir = record.get('wind_dir')\n",
    "        wind_speed = record.get('wind_speed')\n",
    "        air_temp = record.get('air_temp')\n",
    "        rel_humid = record.get('rel_humid')\n",
    "        acres_burn = record.get('acres_burn')\n",
    "        crop_burn1 = record.get('crop_burn1')\n",
    "        crop_burn2 = record.get('crop_burn2')\n",
    "        crop_burn3 = record.get('crop_burn3')\n",
    "        \n",
    "        d = str(date)\n",
    "        if len(d) == 8:\n",
    "            date = str(d[4:]+\"-\"+d[0]+d[1]+\"-\"+d[2]+d[3])\n",
    "            \n",
    "        if len(d) == 7:\n",
    "            date = str(d[3:]+\"-\"+\"0\"+d[0]+\"-\"+d[1]+d[2])  \n",
    "        incident_id = str(incident_id)+\"-\"+date[:4]\n",
    "    \n",
    "        return [(incident_id,state,fdid,date,inc_no,exposure_no,weath_type,wind_dir,wind_speed,air_temp,rel_humid,acres_burn,crop_burn1,crop_burn2,crop_burn3,)]\n",
    "\n",
    "\n",
    "\n",
    "# PTransform: format for BQ sink\n",
    "class MakeRecordFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        incident_id, state, fdid, date, inc_no, exposure_no, weath_type, wind_dir, wind_speed, air_temp, rel_humid, acres_burn, crop_burn1, crop_burn2, crop_burn3 = element\n",
    "        record = {'incident_id':incident_id,\n",
    "                    'state':state,\n",
    "                    'fdid':fdid,\n",
    "                    'date':date,\n",
    "                    'inc_no':inc_no,\n",
    "                    'exposure_no':exposure_no,\n",
    "                    'weath_type':weath_type,\n",
    "                    'wind_dir':wind_dir,\n",
    "                    'wind_speed':wind_speed,\n",
    "                    'air_temp':air_temp,\n",
    "                    'rel_humid':rel_humid,\n",
    "                    'acres_burn':acres_burn,\n",
    "                    'crop_burn1':crop_burn1,\n",
    "                    'crop_burn2':crop_burn2,\n",
    "                    'crop_burn3':crop_burn3}\n",
    "        return [record] \n",
    "    \n",
    "    \n",
    "##FIX TTHIS\n",
    "PROJECT_ID = 'trusty-wavelet-252622'\n",
    "BUCKET = 'gs://wildland_incidents'\n",
    "\n",
    "# Project ID is needed for BigQuery data source, even for local execution.\n",
    "options = {\n",
    "    'runner': 'DataflowRunner',\n",
    "    'job_name': 'transform-state',\n",
    "    'project': PROJECT_ID,\n",
    "    'temp_location': BUCKET + '/temp',\n",
    "    'staging_location': BUCKET + '/staging',\n",
    "    'machine_type': 'n1-standard-4', # machine types listed here: https://cloud.google.com/compute/docs/machine-types\n",
    "    'num_workers': 1\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "# Create a Pipeline using a local runner for execution.\n",
    "with beam.Pipeline('DataflowRunner', options=opts) as p:\n",
    "    \n",
    "    query_string = 'SELECT * FROM WildLand_Incidents_modeled.Incidents'\n",
    "    query_results = p | 'Read from BigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query_string))\n",
    "\n",
    "    # write PCollection to log file\n",
    "    query_results | 'Write to log 1' >> WriteToText('query_results.txt')\n",
    "\n",
    "    # apply a ParDo to the PCollection \n",
    "    pkey_pcoll = query_results | 'Create primary key' >> beam.ParDo(CreatePrimaryKey())\n",
    "\n",
    "    # write PCollection to log file\n",
    "    pkey_pcoll | 'Write File' >> WriteToText('incident_output.txt')\n",
    "    \n",
    "    # make BQ records\n",
    "    bq_pcoll = pkey_pcoll | 'Make BQ Record' >> beam.ParDo(MakeRecordFn())\n",
    "    \n",
    "    qualified_table_name = PROJECT_ID + ':WildLand_Incidents_modeled.Incidents_Beam_DF'\n",
    "    table_schema = 'incident_id:STRING,state:STRING,fdid:STRING,date:DATE,inc_no:STRING,exposure_no:INTEGER,weath_type:STRING,wind_dir:STRING,wind_speed:INTEGER,air_temp:INTEGER,rel_humid:INTEGER,acres_burn:FLOAT,crop_burn1:STRING,crop_burn2:STRING,crop_burn3:STRING'\n",
    "    \n",
    "    bq_pcoll | 'Write to BigQuery' >> beam.io.Write(beam.io.BigQuerySink(qualified_table_name, \n",
    "                                                    schema=table_schema,  \n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
