{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Dataset trusty-wavelet-252622:temp_dataset_47e357de53504938a16a6b5bfb49a4a5 does not exist so we will create it as temporary with location=None\n",
      "WARNING:root:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "# DoFn to perform on each element in the input PCollection.\n",
    "class CollectDateFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        record = element\n",
    "        date = record.get('date')\n",
    "\n",
    "        return [(date, 1)]    \n",
    "\n",
    "# DoFn to perform on each element in the input PCollection.\n",
    "class SplitDateFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        date_id, notneeded = element \n",
    "        date = date_id # must cast to a list in order to call len()\n",
    "        year = date % 10000\n",
    "        rest = date / 10000\n",
    "        day = rest % 100\n",
    "        month = rest / 100\n",
    "        \n",
    "        date_id = str(year)+\"-\"+str(month)+\"-\"+str(day)\n",
    "        \n",
    "        if month == 1:\n",
    "            monthname = \"January\"\n",
    "        elif month == 2:\n",
    "            monthname = \"Febuary\"\n",
    "        elif month == 3:\n",
    "            monthname = \"March\"\n",
    "        elif month == 4:\n",
    "            monthname = \"April\"\n",
    "        elif month == 5:\n",
    "            monthname = \"May\"\n",
    "        elif month == 6:\n",
    "            monthname = \"June\"\n",
    "        elif month == 7:\n",
    "            monthname = \"July\"\n",
    "        elif month == 8:\n",
    "            monthname = \"August\"\n",
    "        elif month == 9:\n",
    "            monthname = \"September\"\n",
    "        elif month == 10:\n",
    "            monthname = \"October\"\n",
    "        elif month == 11:\n",
    "            monthname = \"November\"\n",
    "        elif month == 12:\n",
    "            monthname = \"December\"\n",
    "        else:\n",
    "            monthname = \"\"\n",
    "\n",
    "        return [(date_id, monthname, month, day, year)]  \n",
    "\n",
    "# PTransform: format for BQ sink\n",
    "class MakeRecordFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        date_id, monthname, month, day, year = element\n",
    "        record = {'date':date_id, \n",
    "               'monthname':monthname, \n",
    "               'month':month,\n",
    "               'day':day,\n",
    "               'year':year}\n",
    "        return [record] \n",
    "\n",
    "##FIX TTHIS\n",
    "PROJECT_ID = 'trusty-wavelet-252622'\n",
    "\n",
    "# Project ID is needed for BigQuery data source, even for local execution.\n",
    "options = {\n",
    "    'project': PROJECT_ID\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "# Create a Pipeline using a local runner for execution.\n",
    "with beam.Pipeline('DirectRunner', options=opts) as p:\n",
    "\n",
    "    query_string = 'SELECT * FROM WildLand_Incidents_modeled.Date'\n",
    "    query_results = p | 'Read from BigQuery' >> beam.io.Read(beam.io.BigQuerySource(query=query_string))\n",
    "\n",
    "    # write PCollection to log file\n",
    "    query_results | 'Write to log 1' >> WriteToText('query_results.txt')\n",
    "\n",
    "    # apply a ParDo to the PCollection \n",
    "    date_pcoll = query_results | 'Extract Date' >> beam.ParDo(CollectDateFn())\n",
    "\n",
    "    # write PCollection to log file\n",
    "    date_pcoll | 'Write to log 2' >> WriteToText('date_count.txt')\n",
    "\n",
    "    # apply GroupByKey to the PCollection\n",
    "    group_pcoll = date_pcoll | 'Group by Date' >> beam.GroupByKey()\n",
    "\n",
    "    # write PCollection to log file\n",
    "    group_pcoll | 'Write to log 3' >> WriteToText('group_by_date.txt')\n",
    "  \n",
    "    # apply a ParDo to the PCollection\n",
    "    out_pcoll = group_pcoll | 'Splits Up Date' >> beam.ParDo(SplitDateFn())\n",
    "\n",
    "    # write PCollection to a file\n",
    "    out_pcoll | 'Write File' >> WriteToText('date_output.txt')\n",
    "    \n",
    "    # make BQ records\n",
    "    bq_pcoll = out_pcoll | 'Make BQ Record' >> beam.ParDo(MakeRecordFn())\n",
    "    \n",
    "    qualified_table_name = PROJECT_ID + ':WildLand_Incidents_modeled.Date_Beam'\n",
    "    table_schema = 'date:DATE,monthname:STRING,month:INTEGER,day:INTEGER,year:INTEGER'\n",
    "    \n",
    "    bq_pcoll | 'Write to BigQuery' >> beam.io.Write(beam.io.BigQuerySink(qualified_table_name, \n",
    "                                                    schema=table_schema,  \n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (beam_venv)",
   "language": "python",
   "name": "beam_venv_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
